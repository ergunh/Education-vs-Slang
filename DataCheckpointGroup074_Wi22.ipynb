{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW1-bOAvJSPL"
      },
      "source": [
        "# COGS 108 - Data Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNU1g78fJSPM"
      },
      "source": [
        "# Names\n",
        "\n",
        "- Sathvik Kaliyur\n",
        "- Kalaina Anderes\n",
        "- Helin Ergun\n",
        "- Maricela Vasquez\n",
        "- Nicholas yamashita"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPupc6cvJSPN"
      },
      "source": [
        "<a id='research_question'></a>\n",
        "# Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kpFj-owJSPO"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "How do viral music hits (defined by apperance in the top 50 ratings of popular music) spread (how it propogates from one point to another) into the American Population. In asking how such music spreads we are asking what is the rate of spread over time during the year of 2019, and in what type of areas do music of a given genre tend to spread first based on their level of urbanization (government urbanization index) and college education rate (% of people with bachelors degree or higher). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRavR17eJSPP"
      },
      "source": [
        "# Dataset(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUM2Mj7TJSPP"
      },
      "source": [
        "*Fill in your dataset information here*\n",
        "\n",
        " \n",
        "- Dataset Name: Tweets\n",
        "- Link to the dataset: https://twitter.com\n",
        "- Number of observations: 100,000\n",
        " \n",
        "Scraped tweets that contain slang from top 50 songs\n",
        " \n",
        " \n",
        "- Dataset Name: song lyrics\n",
        "- Link to the dataset: https://genius.com\n",
        "- Number of observations: 50 * 365\n",
        " \n",
        "Lyrics of the top 50 viral songs\n",
        " \n",
        " \n",
        " \n",
        "- Dataset Name: Education data by county\n",
        "- Link to the dataset: https://github.com/COGS108/Group074-Wi22/blob/master/Education%20-%20EducationReport%20(1)%20(1).csv\n",
        "- Number of observations: 3151\n",
        " \n",
        "Education data by county, of who went to college. This dataset contains the college education rate of every county in the United States. It also contains the urbanization index score of each county as well with its associated state.\n",
        " \n",
        "- Dataset Name: Song genre\n",
        "- Link to the dataset: https://spotify.com\n",
        "- Number of observations: 50 * 365\n",
        " \n",
        "The genre of the top 50 songs we scraped\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "If you plan to use multiple datasets, add 1-2 sentences about how you plan to combine these datasets.\n",
        "\n",
        "Using the geocode data from the song lyrics dataset, we plan to correspond each row by the county indicated by its geocode with the matching county on the education dataset. Then we plan to add the corresponding genre of each song into the new consolidated dataset based on the song titles from the lyrics data set wtih the end product containing rows with the song title, song genre, % college educated, and urbanization index by county within their correpsonding state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y7ah1CZJSPP"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0OWVHWJJSPQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Import seaborn and apply its plotting styles\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfP9OhFEJSPQ"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmTDpnW0JSPR"
      },
      "source": [
        "Education Data:\n",
        "The original data was segmented by each state. So we took the data into a google sheet and consolidated all of the data together. Once the data was put together, we imported it into a notebook using pandas, dropped all rows with missing data sets, and restricted it to the specfic columns that we are analyzing for this project. The dataset was quite usable on its own right with all of the datapoints we needed consolidated in a tidy format. We just needed to optimize it for easy analytics. Thus besides the above mentioned filtering of the dataset no transformations of the underlying data was neccesary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WGUMyOleJSPS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "98486ca5-97f6-4446-f927-d7f0c09eb1e4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-12ee707a75fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcsv_ed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Education - EducationReport (1) (1).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0meducation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_ed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meducation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Education - EducationReport (1) (1).csv'"
          ]
        }
      ],
      "source": [
        "sns.set(font_scale=2, style=\"white\")\n",
        "education = pd.read_csv(\"Education - EducationReport (1) (1).csv\")\n",
        "education.head()\n",
        "education = education.dropna(axis = 0)\n",
        "education = education[['County','State','Urban','2019']]\n",
        "education.shape\n",
        "education = education.rename(columns={\"2019\":\"% College\"})\n",
        "education.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Scraping to collect twitter data <h1>"
      ],
      "metadata": {
        "id": "IxYFhBkWP5fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tweets scraped are pretty well organized in the CSV format. We set up the scrape to have things like URL, Like Count, Replay Count all in their own column, The tweets will need to be cleaned. We started cleaning them using excel, so no code yet, but we are removing double, emojis, stop words, and punctuations. Will be using tehcniques like V-look up. We will be doing data transformations like converting the CSV into JSON format in order to do comparisons likes compairng the geo location of the tweet to the the geo location of the county and its education level. We will then need to conver tback to CSV format inorder to plot and create our visulizations. "
      ],
      "metadata": {
        "id": "BhLmA7oCUyWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Set maximum tweets to pull\n",
        "# How many tweets per specific date (set low # - medium #)\n",
        "maxTweets = 10\n",
        "\n",
        "# How many different dates (set high #)\n",
        "maxTweets2 = 3\n",
        "# Set what keywords you want your twitter scraper to pull\n",
        "\n",
        "keyword = \"music\"\n",
        "#Open/create a file to append data to\n",
        "csvFile = open('scraped1.csv', 'a', newline='', encoding='utf8')\n",
        "#Use csv writer\n",
        "csvWriter = csv.writer(csvFile)\n",
        "csvWriter.writerow(['url','date', 'like count','replys','retweets','tweet'])\n",
        "\n",
        "# Write tweets into the csv filter\n",
        "for i in range(maxTweets2):\n",
        "\n",
        "    randomEndYear = random.randint(2012, 2020)\n",
        "    randomEndMonth = random.randint(1, 12)\n",
        "    randomEndDay = random.randint(1, 30)\n",
        "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' lang:en since:2011-01-01 until:{}-{}-{} -filter:links -filter:replies'.format(randomEndYear, randomEndMonth, randomEndDay)).get_items()):\n",
        "\n",
        "            if i > maxTweets :\n",
        "                break\n",
        "            csvWriter.writerow([tweet.url, tweet.date, tweet.likeCount, tweet.replyCount, tweet.retweetCount, tweet.content])\n",
        "\n",
        "\n",
        "\n",
        "csvFile.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "lPjJwsRvMEAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "DataCheckpointGroup074-Wi22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}