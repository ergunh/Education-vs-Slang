{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh9pHYurMXCg"
      },
      "source": [
        "# COGS 108 - EDA Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN4P5AtjMXCk"
      },
      "source": [
        "# Names\n",
        "\n",
        "- Helin Ergun\n",
        "- Maricela Vasquez\n",
        "- Sathvik Kaliyur\n",
        "- Kalaina Anderes\n",
        "- Nicholas Yamashita"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNIWsPltMXCk"
      },
      "source": [
        "<a id='research_question'></a>\n",
        "# Research Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vAFi15xMXCl"
      },
      "source": [
        "Temporally and geographically, we measured as a team how slang (words & terms) originated from popular songs and integrated within everyday language, through Tweets. Beginning from the release date and searching Twitter slang for year 2019, we graphed and measured the increase in Twitter slang usage, geographic spread in U.S. counties, prevalence, and based upon this information made inferences about this correlated relationship. \n",
        "\n",
        "**Abstract:**\n",
        "How do viral music hits (defined by Spotify Top 50 most popular rated) spread (how it propogates from one point to another) into the American Population? In specified music spreads, we analyzed the rate of spread over time during 2019, and what music genres tend to spread primarily due to their urbanization value (government urbanization index) in addition to college education rate (% of people with bachelors degree or higher)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW3aGE4OMXCl"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7099ZhjMXCl"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "# Import seaborn and apply its plotting styles\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=2, style=\"white\")\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", 104)\n",
        "import patsy\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ttest_ind, chisquare, normaltest\n",
        "from scipy.stats import mannwhitneyu\n"
      ],
      "metadata": {
        "id": "i2vGLtdAOt_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDOn9GuMXCn"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHZj-ZsdMXCn"
      },
      "source": [
        "Describe your data cleaning steps here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the 52 CSV's that we had to hand download from SpotifyCharts.com. We also had to make an Automator script to rename all the CSV files to songs_1...songs_52, so that we could efficently read-in the CSV's"
      ],
      "metadata": {
        "id": "TjGUWbTwp981"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "csv_list = []\n",
        "\n",
        "i = 2\n",
        "\n",
        "while i <= 52:\n",
        "    csv_list.append(f'songs_{i}.csv')\n",
        "    i = i + 1\n",
        "\n",
        "print(csv_list)\n",
        "\n",
        "\n",
        "\n",
        "df_master = pd.read_csv('master.csv')\n",
        "\n",
        "\n",
        "for file in csv_list:\n",
        "    df = pd.read_csv(file)\n",
        "    df.to_csv('master.csv', mode = 'a', header = False, index=False)\n"
      ],
      "metadata": {
        "id": "KqTKwIbJp5ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapes Genius.com for lyrics. We got the top 50 viral songs for each week from SpotifyCharts.com into CSV format. We merged all 52 CSV's detailed in the last checkpoint. For scraping lyrics from Genius.com we used the LyricsGenius library. We search the artists then the specific song. For here we use a try except because some of the songs were not found and would throw an error and stop the program. Then dump to a JSON file each loop just incase an unforeseen error would occur and we had to run again and could start from the last data point."
      ],
      "metadata": {
        "id": "zfJjjzcvM1mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lyricsgenius\n",
        "import json\n",
        "\n",
        "# Client Access Token\n",
        "token = \"t19ZRmG1y5jaY-LjD1SEFdLuTB0dDiwQab4SNxWIofs7g8XHt6uJLCX8bnd-dow1\"\n",
        "\n",
        "genius = lyricsgenius.Genius(token)\n",
        "genius.response_format = 'plain'\n",
        "\n",
        "\n",
        "# open json file that is in the same directory\n",
        "with open('master_json.json') as f:\n",
        "    # load json data into a python object\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "# loop through each song/artist and fins lyrics then append to JSON obj\n",
        "for tweets in data['tweets']:\n",
        "\n",
        "    try:\n",
        "        content_artist = tweets['Artist']\n",
        "        content_track = tweets['Track Name']\n",
        "\n",
        "        artist = genius.search_artist(f\"{content_artist}\", max_songs=1, sort=\"title\", include_features=True)\n",
        "        song = artist.song(f\"{content_track}\")\n",
        "\n",
        "\n",
        "        print(song.lyrics)\n",
        "\n",
        "        tweets['Lyrics'] = song.lyrics\n",
        "\n",
        "        # Save\n",
        "        with open('master_lyrics_added.json', 'w') as f:\n",
        "            json.dump(data, f, indent = 2)\n",
        "    except:\n",
        "        continue\n"
      ],
      "metadata": {
        "id": "BZ1GA3cvM0Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partially cleans the lyrics data, removes a few things like \\n, - (this isn't finished as we are still cleaning) For this we used Regex or I guess re more specifically.** We will continue to clean the data by removing stop words and more symbols from the lyric data."
      ],
      "metadata": {
        "id": "yB73mPSgM-MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# open json file that is in the same directory\n",
        "with open('master_lyrics_added.json') as f:\n",
        "    # load json data into a python object\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "# remove the \"\\n\"\n",
        "for tweets in data['tweets']:\n",
        "\n",
        "    try:\n",
        "        content = tweets[\"Lyrics\"]\n",
        "\n",
        "        # regex\n",
        "        pattern = r'\\n'\n",
        "        mod_string = re.sub(pattern, ' ', content)\n",
        "\n",
        "        # append ack to JSON\n",
        "        tweets['Lyrics'] = mod_string\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "\n",
        "# remove the \"-\"\n",
        "for tweets in data['tweets']:\n",
        "\n",
        "    try:\n",
        "        content = tweets[\"Lyrics\"]\n",
        "\n",
        "        # regex\n",
        "        pattern = r'-'\n",
        "        mod_string = re.sub(pattern, ' ', content)\n",
        "\n",
        "        # append ack to JSON\n",
        "        tweets['Lyrics'] = mod_string\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "\n",
        "# remove the \"\\\"\n",
        "for tweets in data['tweets']:\n",
        "\n",
        "    try:\n",
        "        content = tweets[\"Lyrics\"]\n",
        "\n",
        "        # regex\n",
        "        pattern = r\"'\\u2005'\"\n",
        "        mod_string = re.sub(pattern, ' ', content)\n",
        "\n",
        "        # append ack to JSON\n",
        "        tweets['Lyrics'] = mod_string\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('ns_removed.json', 'w') as f:\n",
        "    json.dump(data, f, indent = 2)"
      ],
      "metadata": {
        "id": "cpQAUggOM-f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transforms the CSV to JSON**\n"
      ],
      "metadata": {
        "id": "_hPQV3PKNKbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "csvfile = open('master.csv', 'r')\n",
        "jsonfile = open('master_json.json', 'w')\n",
        "\n",
        "\n",
        "\n",
        "fieldnames = ('Position', 'Track Name', 'Artist')\n",
        "reader = csv.DictReader( csvfile, fieldnames)\n",
        "for row in reader:\n",
        "    json.dump(row, jsonfile, indent=4)\n",
        "    jsonfile.write(',\\n')\n"
      ],
      "metadata": {
        "id": "SIU0ixY2NKjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Genius returns a bunch of data features we don't need, so part of cleaning we removed a bunch of values. ( The Key says tweets becuase Code was reused from the twitter scraper)**"
      ],
      "metadata": {
        "id": "1eVkzXq3Nm7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# open json file that is in the same directory\n",
        "with open('lyrics.json') as f:\n",
        "    # load json data into a python object\n",
        "    data = json.load(f)\n",
        "# loop through 'tweets' looking for certin keys\n",
        "for tweets in data['tweets']:\n",
        "    #print(tweets['like count'], tweets['url'])\n",
        "    del tweets['_type']\n",
        "    del tweets['annotation_count']\n",
        "    del tweets['api_path']\n",
        "    del tweets['full_title']\n",
        "    del tweets['header_image_thumbnail_url']\n",
        "    del tweets['header_image_url']\n",
        "    del tweets['id']\n",
        "    del tweets['instrumental']\n",
        "    del tweets['lyrics_owner_id']\n",
        "    del tweets['lyrics_state']\n",
        "    del tweets['lyrics_updated_at']\n",
        "    del tweets['path']\n",
        "    del tweets['pyongs_count']\n",
        "    del tweets['song_art_image_thumbnail_url']\n",
        "    del tweets['song_art_image_url']\n",
        "    del tweets['stats']\n",
        "    #del tweets[]['unreviewed_annotations']\n",
        "    #del tweets['hot']\n",
        "    #del tweets['pageviews']\n",
        "    del tweets['title_with_featured']\n",
        "    del tweets['updated_by_human_at']\n",
        "    del tweets['url']\n",
        "    del tweets['primary_artist']\n",
        "    del tweets['artist']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('lyrics_update.json', 'w') as f:\n",
        "    json.dump(data, f, indent = 2)\n"
      ],
      "metadata": {
        "id": "1AU0pRLsNnBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Education Data** data source: https://data.ers.usda.gov/reports.aspx?ID=17829"
      ],
      "metadata": {
        "id": "rL537oyXOtkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "education = pd.read_csv(\"Education - EducationReport (1) (1).csv\")\n",
        "education.dropna(axis='rows')\n",
        "education.head()\n",
        "education.describe()\n"
      ],
      "metadata": {
        "id": "-TlHH2ZfOxlj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4af49112-e715-4342-b21c-f48e9fcab6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5f5488cd46f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meducation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Education - EducationReport (1) (1).csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0meducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Education - EducationReport (1) (1).csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above Cleaning discussed in Checkpoint 1 "
      ],
      "metadata": {
        "id": "RIgYRJ8Fd5fM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpLNsNeMXCo"
      },
      "source": [
        "# Data Analysis & Results (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoTNhCsgMXCp"
      },
      "source": [
        "Carry out EDA on your dataset(s); Describe in this section"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jR6Sosfudgv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph takes the distribution of urban index scores vs college education rate for every state that had a significant relationship between the two factors. There are no outliers. However no consistent trends into what is causing signficance in each state. Generally, they are negativley correlated, where the higher the score, the lower the college rate, but that does not hold for every state. We learn that the states that do have signifcant correlation between these two variables see a negative relationship. "
      ],
      "metadata": {
        "id": "lcYybCM4Sj5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "part = education['State'].unique().tolist()\n",
        "corr_dict = {}\n",
        "for i in part:\n",
        "    df = education[(education.State == i)]\n",
        "    #sns.barplot(x=\"Urban\",y=\"2019\",data = df)\n",
        "    column_1 = df['Urban']\n",
        "    column_2 = df[\"2019\"]\n",
        "    correlation = column_1.corr(column_2)\n",
        "    if(abs(correlation)>.5):\n",
        "        corr_dict[i] = correlation\n",
        "corr_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf7ThAkNO2Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listy = corr_dict.keys()\n",
        "df2 = education[education['State'].isin(listy)]\n",
        "df2\n",
        "g = sns.catplot(\n",
        "    data=df2, kind=\"bar\",\n",
        "    x=\"State\", y=\"2019\",hue = 'Urban',height=25,ci = None)\n",
        "\n",
        "plt.ylabel(\"% College\")\n",
        "plt.title(\"Singifcant Urban College Correlations\")"
      ],
      "metadata": {
        "id": "PHtne3wYO4Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph takes the college education rates of the least college educated counties in the country by urbanization index. There are no outliers, and no consistent relationship either. All of the counties plotted have Index scores greater than 4, but there is still signifcant variation after that. This graph allows us to get a general understanding of the types of counties at the bottom of the college education spectrum and learn taht these counties are generally in more rural comunities. "
      ],
      "metadata": {
        "id": "KP0nTaOLT4fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = education.sort_values(by=['2019'],ascending=False)\n",
        "df = df[df['2019']<=.08]\n",
        "part = df['County'].unique().tolist()\n",
        "g = sns.catplot(\n",
        "    data=df, kind=\"bar\",\n",
        "    x=\"County\", y=\"2019\",height=45,hue = 'Urban', ci = None)\n",
        "plt.ylabel(\"% College\")\n",
        "plt.title(\"Least College Educated\")"
      ],
      "metadata": {
        "id": "Hl0_sTu7O59E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph takes the college education rates of the most college educated counties in the country by urabnization index. There are no outliers, and it seems that the vast majority of these counties have an urbanization index of 1, but other urban index scores appear on the graph as well distorting any potnetial correlation. This graph allows us to vizualize that many of the top counties are in major metropolitan areas like New York or DC. "
      ],
      "metadata": {
        "id": "2YK6rsj6UWBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df = education.sort_values(by=['2019'],ascending=False)\n",
        "df = df[df['2019']>=.58]\n",
        "part = df['County'].unique().tolist()\n",
        "g = sns.catplot(\n",
        "    data=df, kind=\"bar\",\n",
        "    x=\"County\", y=\"2019\",height=45,hue = 'Urban', ci = None)\n",
        "plt.ylabel(\"% College\")\n",
        "plt.title(\"Most College Educated\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8WRB-sZyQeXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph takes in the college education rates of the 75th, 50th, and 25th percentile of counties in terms of 2019 college education rate. There are no outliers, but this graph shows that thoguh all these categories increased overtime, the 75th percentile increased the most with the largest growth rate. This shows that like many other aspects of American society, the most educated counties have been becoming more and more educated, where as the rest of the country is progressing a lot slower. "
      ],
      "metadata": {
        "id": "nSFi8zrWZ-3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sns.histplot(x=\"2019\",data = education,bins =5,color='grey')\n",
        "df2 = education[education['2019']<=.15]\n",
        "df = education[education['2019']>=.3]\n",
        "df3 = education[education['2019']<.3]\n",
        "df3 = df3[df3['2019']>.15]\n",
        "med_list = []\n",
        "e_list = []\n",
        "n_list = []\n",
        "year_list = ['1970','1980','1990','2000','2019']\n",
        "for i in year_list:\n",
        "    med_list.append(df3[i].median())\n",
        "for i in year_list:\n",
        "    e_list.append(df[i].median())\n",
        "for i in year_list:\n",
        "    n_list.append(df2[i].median())\n",
        "med_list\n",
        "sns.lineplot(x=year_list,y=med_list)\n",
        "sns.lineplot(x=year_list,y=e_list)\n",
        "sns.lineplot(x=year_list,y=n_list)\n",
        "plt.title(\"College Education Overtime\")"
      ],
      "metadata": {
        "id": "_yzvXY8xQp1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph takes in the college education rate for every county as a histogram. The data is clearly skewed right, with a few highly educated counties in the tail of the distrbution. There is no relationship that one can extrapolate soley based on the graph as it is a univariate histogram, but it does show how college education rates are distributed acorss all of America's counties."
      ],
      "metadata": {
        "id": "biHoZP5Ta50I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = sns.histplot(x=\"2019\",data =education,bins = 40)\n",
        "plt.xlabel(\"% College Educated by County\")\n",
        "p.set_title(\"College Education Distribution\")"
      ],
      "metadata": {
        "id": "o3ygbxvCQ3S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph looks at urban index scores by college education rate across all counties in the country. There do not seem to be major outliers, and no clear relationship. Generally, college education rates decrease as urban index score increases, but that is not unviersally true. However, counties with an urban index score of 1.0 tend to have significantly larger college rates than other counties. This graph lets us grasp the general trend of how education and urbanness interact with each other"
      ],
      "metadata": {
        "id": "4JnyM6vDbyMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "p= sns.barplot(x=\"Urban\",y=\"2019\",data = education)\n",
        "p.set_xlabel(\"Urban Index Score\")\n",
        "p.set_ylabel(\"% College\")\n",
        "p.set_title(\"% College by Urban Index\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XY63Xmf7Q5FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph analyzes the distribtuion of college education rates by time period examing the distribtuion in 1970, 1980, 1990, 2000, and 2019. Each distribution is more evenly distributed then their predeccsor, with 1970 heavily peaking close to 0%, while 2019 has a much lower peak at near 20%. The datasets are normally distirbuted for each year examined so there are outliers at the edges. But overall, this data shows that the country has been becoming more educated over time. And the wider level of distribution indicates a level of new vairation in level of change, and a less uniform distribtuion at large. This graph allows us to see how the college education in America has changed overtime."
      ],
      "metadata": {
        "id": "2m2BUDAbcb1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(x=\"1970\",data = education,bins =90)\n",
        "sns.histplot(data=education, x=\"1980\",bins = 90, color=\"red\")\n",
        "sns.histplot(data=education, x=\"1990\",bins = 90, color=\"grey\")\n",
        "sns.histplot(data=education, x=\"2000\",bins = 90, color=\"darkblue\")\n",
        "sns.histplot(data=education, x=\"2019\",bins = 90, color=\"green\")\n",
        "plt.xlabel(\"% with College Degree\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"College Education Trends\")"
      ],
      "metadata": {
        "id": "Q0cCdc2SQ6nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TLgm-DnmQ8UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh_lMK0bMXCp"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE\n",
        "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1_4bswBZQpUx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "EDACheckpointGroup074-Wi22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}